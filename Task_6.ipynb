{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Task_6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/infox182/Document-Classification/blob/master/Task_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JaUbaNPDXFi"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
        "- `news_train.txt` тренировочное множество\n",
        "- `news_test.txt` тренировочное множество\n",
        "\n",
        "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
        "\n",
        "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
        "\n",
        ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWJIshWzDXFt"
      },
      "source": [
        "# Задача\n",
        "\n",
        "1. Обработать данные, получив для каждого текста набор токенов\n",
        "Обработать токены с помощью (один вариант из трех):\n",
        "    - pymorphy2\n",
        "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
        "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
        "    \n",
        "    \n",
        "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
        "\n",
        "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
        "     - SVM\n",
        "     - наивный байесовский классификатор\n",
        "     - логистическая регрессия\n",
        "    \n",
        "\n",
        "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsOKjP-dDXFv"
      },
      "source": [
        "lines = list(open('./news_train.txt', 'r', encoding='utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdnmHKRoESY1"
      },
      "source": [
        "import nltk\r\n",
        "!pip install pymorphy2\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bc6tck8DXFw"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymorphy2\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.stem.snowball import RussianStemmer \n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utvOtPEFDXFx"
      },
      "source": [
        "# train_df = pd.read_table('../data/news/news_train.txt',header=None,names = ['label','head','text'])\n",
        "# test_df = pd.read_table('../data/news/news_test.txt',header=None,names = ['label','head','text'])\n",
        "train_df = pd.read_table('/content/drive/MyDrive/Colab Notebooks/CL_DOC/news/news_train.txt',header=None,names = ['label','head','text'])\n",
        "test_df = pd.read_table('/content/drive/MyDrive/Colab Notebooks/CL_DOC/news/news_test.txt',header=None,names = ['label','head','text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "b9frRaMvDXFy",
        "outputId": "5deac11d-be44-450f-9fba-7913464b065b"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>head</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sport</td>\n",
              "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
              "      <td>Нападающий «Вашингтон Кэпиталз» Александр Овеч...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>culture</td>\n",
              "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
              "      <td>Власти Мексики объявили подделкой статую майя,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>science</td>\n",
              "      <td>Samsung представила флагман в защищенном корпусе</td>\n",
              "      <td>Южнокорейская Samsung анонсировала защищенную ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n",
              "      <td>Контрольно-дисциплинарный комитет (КДК) РФС сн...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>media</td>\n",
              "      <td>Hopes &amp; Fears объединится с The Village</td>\n",
              "      <td>Интернет-издание Hopes &amp; Fears объявило о свое...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     label  ...                                               text\n",
              "0    sport  ...  Нападающий «Вашингтон Кэпиталз» Александр Овеч...\n",
              "1  culture  ...  Власти Мексики объявили подделкой статую майя,...\n",
              "2  science  ...  Южнокорейская Samsung анонсировала защищенную ...\n",
              "3    sport  ...  Контрольно-дисциплинарный комитет (КДК) РФС сн...\n",
              "4    media  ...  Интернет-издание Hopes & Fears объявило о свое...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf6gmHxEDXF0",
        "outputId": "b63e609f-5fff-4e5f-861a-79970a1e8452"
      },
      "source": [
        "train_df.groupby('label').count()['head']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "business      554\n",
              "culture      2053\n",
              "economics    2080\n",
              "forces       1225\n",
              "life         2033\n",
              "media        2111\n",
              "science      2156\n",
              "sport        2215\n",
              "style         284\n",
              "travel        289\n",
              "Name: head, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jFzEyy6DXF1"
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "SW = stopwords.words('russian')\n",
        "SW.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', '–', 'к', 'на', '...'])\n",
        "PU = string.punctuation\n",
        "PU += '«»–—``''\"\"'\n",
        "PU += \"''\"\n",
        "def tokenize_and_clean(text):\n",
        "    text_tokens = []\n",
        "    text = text.replace('.','. ')\n",
        "    for sent in sent_tokenize(text, 'russian'):\n",
        "        tokens = word_tokenize(sent)\n",
        "        for token in tokens:\n",
        "            token = morph.parse(token.lower())[0].normal_form\n",
        "            if (re.match(r'[а-я]',token) is not None) and (len(token)>=3)\\\n",
        "                and (token not in SW) and (token not in PU):\n",
        "                text_tokens.append(token)\n",
        "    return text_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxQeEqu9DXF2"
      },
      "source": [
        "test_texts = list(test_df['text'])\n",
        "test_labels = list(test_df['label'])\n",
        "train_texts = list(train_df['text'])\n",
        "train_labels = list(train_df['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koGuKHXgDXF2"
      },
      "source": [
        "test_tokenize_texts = []\n",
        "train_tokenize_texts = []\n",
        "for i,text in enumerate(test_texts):\n",
        "    test_tokenize_texts.append(tokenize_and_clean(text))\n",
        "for i,text in enumerate(train_texts):\n",
        "    train_tokenize_texts.append(tokenize_and_clean(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAK9kkisDXF3"
      },
      "source": [
        "#обучение word2vec\r\n",
        "model = gensim.models.Word2Vec(train_tokenize_texts, size=250, window=5, min_count=5, workers=4)\r\n",
        "model.save('/content/drive/MyDrive/Colab Notebooks/CL_DOC/w2v.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co0GcF6BHxfq",
        "outputId": "624074ee-f926-4c9e-c4df-f66113c8d1fa"
      },
      "source": [
        "model.most_similar('мяч')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('гол', 0.9651415348052979),\n",
              " ('забитый', 0.9593526721000671),\n",
              " ('забить', 0.9579737186431885),\n",
              " ('ворота', 0.9438250064849854),\n",
              " ('дубль', 0.9358864426612854),\n",
              " ('пас', 0.932077944278717),\n",
              " ('шайба', 0.9283113479614258),\n",
              " ('безответный', 0.923426628112793),\n",
              " ('буссуфа', 0.9129296541213989),\n",
              " ('победный', 0.9125521779060364)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ZY77RnKM2_",
        "outputId": "d641d2d2-cc7b-4764-96ee-0ddd478ed807"
      },
      "source": [
        "model.most_similar('наука')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ран', 0.8451852202415466),\n",
              " ('культурный', 0.827465295791626),\n",
              " ('медицина', 0.8135280013084412),\n",
              " ('образование', 0.7858641743659973),\n",
              " ('деятель', 0.7858037352561951),\n",
              " ('институт', 0.7847318053245544),\n",
              " ('академия', 0.7828506827354431),\n",
              " ('мгу', 0.7737308144569397),\n",
              " ('журналистика', 0.769000232219696),\n",
              " ('везикулярный', 0.7676552534103394)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xJtzkVzKG9R"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eITEu_SBMYm-"
      },
      "source": [
        "# #feature_sel если обучать нейронками\r\n",
        "# def text_to_vec(text,model):\r\n",
        "#   vec = []\r\n",
        "#   for word in text:\r\n",
        "#     try:\r\n",
        "#       extract = model.wv.get_vector(f'{word}')\r\n",
        "#       vec.append(extract)\r\n",
        "#     except KeyError:\r\n",
        "#       continue\r\n",
        "#   return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Ddb8iCbCMZ"
      },
      "source": [
        "label_dict = {label:i for i, label in enumerate(list(train_df['label'].unique()))}\r\n",
        "\r\n",
        "def fun(doc):\r\n",
        "    return doc\r\n",
        "tfidf_vectorizer = TfidfVectorizer(\r\n",
        "    analyzer='word',\r\n",
        "    tokenizer=fun,\r\n",
        "    preprocessor=fun,\r\n",
        "    token_pattern=None,\r\n",
        "    max_features = 9000)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qOUYRDKbGwk"
      },
      "source": [
        "X_train = tfidf_vectorizer.fit_transform(train_tokenize_texts)\r\n",
        "y_train = np.array(train_df['label'].apply(lambda label: label_dict[label]))\r\n",
        "X_test = tfidf_vectorizer.transform(test_tokenize_texts)\r\n",
        "y_test = np.array(test_df['label'].apply(lambda label: label_dict[label]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhTkatkeiY1R"
      },
      "source": [
        "def svm_param_selection(X, y):\r\n",
        "    Cs = [0.001, 0.01, 0.1, 1, 10]\r\n",
        "    gammas = [0.001, 0.01, 0.1, 1]\r\n",
        "    param_grid = {'C': Cs, 'gamma' : gammas}\r\n",
        "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=3)\r\n",
        "    grid_search.fit(X, y)\r\n",
        "    # print(grid_search.best_params_)\r\n",
        "    return grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjp1wGpvsDz0"
      },
      "source": [
        "svm_best_params = svm_param_selection(X_train,y_train) #too long time"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vQR6mx5sX-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0932528b-e4e4-434f-df28-9c57d0b57d9d"
      },
      "source": [
        "svm = SVC(kernel='rbf', C=svm_best_params['C'], gamma=svm_best_params['gamma'])\r\n",
        "svm.fit(X_train, y_train)\r\n",
        "accuracy_train = np.mean(y_train == svm.predict(X_train))\r\n",
        "accuracy_test  = np.mean(y_test  == svm.predict(X_test))\r\n",
        "print(f'accuracy svm train: {accuracy_train}', '\\n',f'accuracy svm test: {accuracy_test}')"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy svm train: 0.9908 \n",
            " accuracy svm test: 0.885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKsf217pbNnn"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyCpG-p4tg-7"
      },
      "source": [
        "def log_reg_param_selection(X, y):\r\n",
        "    Cs = [0.001, 0.01, 0.1, 1, 10]\r\n",
        "    param_grid={'C': Cs}\r\n",
        "    grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=3)\r\n",
        "    grid_search.fit(X, y)\r\n",
        "    # print(grid_search.best_params_)\r\n",
        "    return grid_search.best_params_\r\n",
        "\r\n",
        "log_reg_best_params = log_reg_param_selection(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfgDC0jWvlJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3615fd2d-84e2-4730-b24f-9c1131e5af4e"
      },
      "source": [
        "log_reg = LogisticRegression(C=log_reg_best_params['C'])\r\n",
        "log_reg.fit(X_train, y_train)\r\n",
        "accuracy_train = np.mean(y_train == log_reg.predict(X_train))\r\n",
        "accuracy_test  = np.mean(y_test  == log_reg.predict(X_test))\r\n",
        "print(f'accuracy logistic regression train: {accuracy_train}', '\\n',f'accuracy logistic regression test: {accuracy_test}')"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy logistic regression train: 0.9980666666666667 \n",
            " accuracy logistic regression test: 0.8773333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}